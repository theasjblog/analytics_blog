{
  "hash": "e93f27a189376713549ef265b0f26319",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Build an end-to-end MLOps solution with vetiver for R and Python - Part 1\"\ndescription: \"How can we build an end-to-end MLOps solution with vetiver? And should we do it?\"\ndate: \"2023-04-13\"\ncategories: [\"mlops\"]\n---\n\n\n![Photo by Max Harlynking on Unsplash.](img/cover.png)\n\n\n### Learning objectives\n\nThe goal of this article series is to answer two questions:\n\n> Can we create a complete MLOps solution with vetiver?\n\n> If so, should we use it?\n\nWe’ll answer those questions by creating a toy example. Although everything will be done in R, the same tools and platforms can be used with Python.\n\n### Introduction\n\nI previously wrote an article about using [vetiver as an MLOps solution](../2023-03-16_vetiver_for_mlops/index.qmd). I concluded that `{vetiver}` is a viable option only when dealing with a limited number of models and with no strict governance requirements.\n\nWhile writing that article, I searched for documentation about an end-to-end R-only MLOps set-up based on `{vetiver}`. I found a few resources, but they all focus on the model development, rather than on the deployment and monitoring.\n\nSo I created my own toy example.\n\nBefore we start discussing any technical details, let’s first recap MLOps and the `{vetiver}` package.\n\n### MLOps\n\nMLOps is a behavioural approach to the modelling work that aims to automate the deployment of quality models to production.\n\nDevOps can teach us what we need to automate deployment successfully. We will need to:\n\n*   Version control our artifacts: code, data, and models.\n*   Automate model governance: who approves which steps and how they do that.\n*   Automate model building.\n*   Automate model testing.\n*   Automate model deployment.\n*   Automate model validation.\n*   Constantly monitor the deployed model, and log and report any significant activity (i.e., any failure in the deployment pipeline or any data drift for the model).\n\nNote that there are some key aspects left out from this list. We’re not looking at things like autoML, auto-(re)training, or challenger/champion scenarios, advanced deployment strategy. We’ll delve into those topics further in the final article of the series.\n\n### Vetiver\n\n`{vetiver}` is a relatively new R and Python package that, according to its own documentation page, aims to enable MLOps within these two languages. Metapackages such as `{tidyverse}` and `{tidymodels}` allows to build a model, and now with `{vetiver}` we can deploy those models.\n\nIn a nutshell, `{vetiver}` offers tools to:\n\n*   Add metadata and documentation to the model.\n*   Version control the model through `{pins}`.\n*   Easily deploy the model to Posit Connect or docker.\n*   Create a monitoring dashboard.\n\nAll those functionalities are great, and they work very well when we look at them in isolation. But what about building a complete MLOps solution using them?\n\nThis series of articles will explore this MLOps option.\n\n### Key constraint for this toy experiment\n\nWe have one main technical constraint: everything must be done in R and using Posit products. The only exceptions will be the version control system we use for our code base.\n\nThis might look like a strange constraint in MLOps, where everything is governed by ad-hoc tools and Python. However, if somebody is seriously considering `{vetiver}` as an MLOps solution, it’s very likely that all the staff available to develop and maintain such a solution are R developers, likely the modellers themselves. They will not be trained ML engineers or DevOps engineers. For this reason, I think that putting this constraint on the solution is valid. We will discuss the implications in the final article of this series.\n\n### How to use this article\n\nThis is a series of four articles. They will be structured as follows:\n\n1.  Introduction to the MLOps problem and the model used.\n2.  Deployment.\n3.  Monitoring.\n4.  Wrap up.\n\nA [repository](https://github.com/theasjblog/vetiverMLOps) is attached to this series of articles. The repository will be populated in sync with the article. That means if you look at it right now, you won’t find the monitoring piece yet. That will come with the third article. This is to avoid confusion due to the presence of code I haven’t explained here yet.\n\nIn each article, we will consider these aspects:\n\n*   The technical implementation of our toy example.\n*   What should be done differently in a real-life case.\n*   How our solution performs as an MLOps option, and if and when we should consider something different.\n\n### The solution building blocks\n\nIn an MLOps solution, you need a few technical parts to work together:\n\n*   Code.\n*   Data.\n*   Deployment.\n*   Monitoring.\n*   Logs.\n\nIn this section of the article, we will look at how we can design a system to connect all these moving parts.\n\n### Solution overview\n\nWe saw what the main three parts are. Let’s now look at how they work with each other.\n\nThe solution design is quite simple, but it serves our purpose.\n\n![Overview of our solution. Storage is provided by pins. The pipeline will be an R Markdown, and the monitoring app will be developed with Shiny. All those components will live in rsconnect (light blue area). The only component outside of rsconnect will be the repository for the code.](./img/solution_design.png)\n\nLet’s analyze the diagram.\n\n### Hosting environment\n\nThe big pale blue block represents parts of the solution that are within the R ecosystem. This can be entirely rsconnect/workbench. You wouldn’t run any of those components locally in a production scenario, but the modellers could develop the model in RStudio local, depending on any data constraint. In this toy example, you can run the entire process locally.\n\nThe two elements outside the pale blue area are version control (you can use whichever tool you like for this, ideally something that enables automatic triggering of a pipeline like Github Actions), and the user calling our API. Since the API is exposed to the network, the user can be any person or system anywhere, as long as they have access to the API itself.\n\n### Pins\n\nWe use `{pins}` to store and version any non-code artifacts: the raw data, the processed data, and the model itself. This is probably not something you can or want to do in a big organization. We’ll discuss this more in-depth at the end of this article.\n\n### Deployment\n\nThe deployment is, in fact, an R markdown. The R markdown can be triggered to run automatically based on Git actions, but in this toy example, we’ll trigger it manually. The deployment script produces three class or artifacts:\n\n*   A report that can be explored to visualize the results of the last successful deployment.\n*   Logs for every attempted deployment.\n*   A model deployed as a `{vetiver}` REST API (an implementation of a `{plumber}` API).\n\n### Logs\n\nAll logs produced by this solution are stored in the server hosting rsconnect/workbench. In our toy example, they’ll be stored locally. Logs are stored in a central folder, with sub-folders identifying the process that created them (deployment, model calls, etc.). Each process creates one log file per day. This might work for you, it might not, depending on the expected volume of calls your model is receiving.\n\n### Monitoring\n\nOur monitoring is a `{Shiny}` dashboard. Here I completely discarded the `{vetiver}` option. `{vetiver}` gives you a `{flexdashboard}`, not a `{Shiny}` app. In my opinion, `{flexdashboard}` is not suited for production-grade dashboards. It has too many limitations and it doesn't promote any good practice at all. The `{Shiny}` I built is not production-grade either, but it’s better than a `{flexdashboard}`. We’ll review this topic at the end of our series.\n\n### API\n\nThis is extremely simple. It's a thin REST API wrapper that `{vetiver}` creates using plumber, and that simply takes in the request for the model and gives the prediction in return. I modified the `plumber.R` file to add logging capabilities and a ping endpoint. Those are missing in the basic implementation from vetiver but are fundamental to getting information about what is going on with the model at the deployment and monitoring stages.\n\n### Project Set up\n\nBefore we can begin working on our project, we need to complete a few set-up steps. For this revision, I will assume that you have cloned the repository into a new R project.\n\nFirst, we need to restore the library by running the following command: `renv::restore()`.\n\nNow, we need to start populating the local board.\n\n**_IMPORTANT NOTE:_**\n\nPlease note that there are steps in our workflow that will completely erase the local board. If you are already using it for something else, make sure you specify a different board for this project.\n\nHere is the code for the set-up script:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(here::here('model_dev', 'R', '00_prepare_raw_data.R'))\n\nlibrary(tidyverse)  \nlibrary(pins)  \nlibrary(nycflights13)\n\n# Clean up local board  \nboard <- pins::board_local(versioned = TRUE)  \nallPins <- board %>%  pins::pin_list()  \nfor (i in allPins) {  \n  board %>%  \n    pins::pin_delete(i)  \n}\n\nboard %>%  \n  pins::pin_write(x = nycflights13::flights,  \n                  name = 'raw_data')\n\nboard %>%  \n pins::pin_write(x = nycflights13::weather,  \n                 name = 'weather')\n```\n:::\n\n\n\nThe above code sets up an empty local board for our `pins` and stores some data from the `{nycflights13}` package onto the board.\n\n### The model\n\nBuilding a model is going to be the easy part. We assume that the project has been approved by all stakeholders, and we can just develop it (we will discuss later what this assumption means for the MLOps process).\n\nWe will be using the same toy model that you can find in [this tidymodels documentation](https://www.tidymodels.org/start/recipes/). The only difference is that we split the data into three sets: a training set, a test set, and a keep-out set that we will use to mock real data in the monitoring phase.\n\nHere is the code for the model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(pins)\n\n# Assume you've run the 00_prepare_raw_data.R script.  \nboard <- board_local(versioned = TRUE)  \nflights <- board %>%  \n  pins::pin_read(name = 'raw_data')  \nweather <- board %>%  \n  pins::pin_read(name = 'weather')\n\n# Prepare the data  \nlate_threshold_min <- 30  \nflight_data <- flights %>%  \n  mutate(  \n  # Convert the arrival delay to a factor  \n  arr_delay = ifelse(arr_delay >= late_threshold_min, \"late\", \"on_time\"),  \n    arr_delay = factor(arr_delay),  \n    # Use the date (not date-time) in the recipe below  \n    date = lubridate::as_date(time_hour)  ) %>%  \n    # Include the weather data  \n    inner_join(weather, by = c(\"origin\", \"time_hour\")) %>%  \n    # Only retain the specific columns we will use  \n    select(dep_time, flight, origin, dest, air_time, distance,  \n           carrier, date, arr_delay, time_hour) %>%  \n    # Exclude missing data  \n    na.omit() %>%  \n    # For creating models, it's better to have qualitative columns  \n    # encoded as factors (instead of character strings)  \n    mutate_if(is.character, as.factor)\n\n# Fix the random numbers by setting the seed  \n# This enables the analysis to be reproducible  \n# when random numbers are used  \nset.seed(222)\n\n# Create keep-out data to be used as mock real data  \nsetdata_split <- initial_split(flight_data, prop = .99)  \ntrain_data <- training(data_split)  \nkeep_out  <- testing(data_split)\n\n# Create training/test datasets  \ndata_split <- initial_split(train_data, prop = 3/4)  \ntrain_data <- training(data_split)  \ntest_data  <- testing(data_split)\n\n# Define the recipe  \nflights_rec <- recipe(arr_delay ~ ., data = train_data) %>%  \n  update_role(flight, time_hour, new_role = \"ID\") %>%  \n  step_date(date, features = c(\"dow\", \"month\")) %>%  \n  step_holiday(date,  \n               holidays = timeDate::listHolidays(\"US\"),  \n               keep_original_cols = FALSE) %>%  \n  step_dummy(all_nominal_predictors()) %>%  \n  step_zv(all_predictors())\n\n# Define the model  \nlr_mod <- logistic_reg() %>%  \n  set_engine(\"glm\")\n\n# Workflow  \nflights_wflow <- workflow() %>%  \n  add_model(lr_mod) %>%  \n  add_recipe(flights_rec)\n\n# Fit  \nflights_fit <-  flights_wflow %>%  \n  fit(data = train_data)\n\n# Pin the data  \nboard %>%  \n pins::pin_write(x = train_data,  \n                 name = 'train_data')\n\nboard %>%  \npins::pin_write(x = test_data,  \n                name = 'test_data')\n\nboard %>%  \n pins::pin_write(x = keep_out,  \n                 name = 'keep_out')\n```\n:::\n\n\n\nAs you can see, this is a simple model. This is fine. This series of articles is not about building a good model; it’s about building a framework to ensure model quality in production. In this context, the model is a cog in a much bigger machine. If you have a better model, simply replace this toy model with yours. As long as you respect the general architecture requirements, it does not matter what model you have.\n\n### Coming Up\n\nIn the next article of this series, we will dive into the core of the project: deploying our model.\n\nWe will put together a markdown to rebuild the model that the modeller prepared, validate it, and deploy it as a REST API.\n\n### Resources\n\n* [Repository](https://github.com/theasjblog/vetiverMLOps)\n* [My previous article on vetiver](https://medium.com/@adrian.joseph/r-and-python-vetiver-package-a-suitable-mlops-solution-111f85f62a41)\n* [vetiver documentation](https://vetiver.rstudio.com/)\n* [The model](https://www.tidymodels.org/start/recipes/)",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}